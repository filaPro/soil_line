\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{color}
\setlist{nolistsep}
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}
\setlength{\parskip}{0.7em}
\setlength{\parindent}{0em}

\begin{document}

\tableofcontents
\newpage


\section{Введение.}


\section{Постановка задачи.}

\subsection{Постановка задачи классификации.}

\par
Напомним определение задачи классификации. 
Пусть $\mathcal{X}$ - множество описаний объектов, 
$\mathcal{Y}$ - конечное множество меток классов, и 
существует неизвестная целевая зависимость - отображение 
$y^\star:\ \mathcal{X} \to \mathcal{Y}$, 
значения которой известны только на конечном подмножестве объектов
$x_1, ..., x_n \in X$. Пары объектов и ответов $(x_i, y_i)$ называют прецендентами.
Совокупность таких пар ${(x_i, y_i)}_{i=1}^n$ называют обучающей выборкой.
\par
Задача обучения по прецендентам заключается в восстановлении зависимости $y^\star$
по заданной обучающей выборк, т.е. в построении решающей функции 
$\mathcal{X} \to \mathcal{Y}$, которая бы
приближала целевую функцию $y^\star(x)$ причем не только на объектах обучающей выборки,
но и на всем множестве $\mathcal{X}$. Кроме того, решающая функция должно допускать эффективную
реализацию на вычислтельой системе.
\par
Каждый объект $x_i$ задается измерениями своих характеристик $f_j(x_i)$, 
которые называют признаками. Таким образом, признаковое описание задается набором функций
$f_j:\ \mathcal{X}\to D_{f_j}$, где $D_{f_j}$ - множество допустимых значений признака.
Выделяют несколько типов признаков в зависимости от множества $D_f$:
\begin{itemize} 
\item бинарный - $D_f=\{0,1\}$
\item номинальный - $D_f$ - конечное множество,
\item порядковый - $D_f$ - конечное упорядоченной множество,
\item количественный - $D_f$ - множество действительных чисел.
\end{itemize}
Пусть признаков $m$ штук, тогда признаковым описание каждого объекта 
$x_i \in \mathcal{X}$ служит вектор $(x_i^1, ..., x_i^m)$. 
Таким образом обучающая выборка представляется в виде совокупности матрицы 
$X \in \mathbb{R}^{n \times m}$ и вектора $Y \in \{1, ..., k\}^n$, где 
$k$ - количество классов.

\subsection{Постановка задачи классификации типов почв по космическим снимкам.}

\par
Основной целью данной работы является решение задачи пространственной классификации
по типам почв некоторой территории, заданной набором её космических снимков.
\par
Как было покаано в предыдущем разделе, входными параметрами задачи классификации являются
признаковое пространство объектов, множество меток классов и обучающая выборка выборка,
составленная из пар элементов данных двух множеств. В качестве объектов рассматриваются точки
поверхности Земли, заданные георграфическими координатами. Признаковое пространство определеяется
из данных космических снимков, покрывающих исследуемую территорию. Метки классов берутся
из результатов наземных исследований территории почвоведами.
\par
В данной работе также исследуется информативность некоторых признаков и моделей, 
которые могут быть использованы не только для задачи классификации, 
но и для других приложений из областипочвоведения. 
Подробнее об этом показано в опубликованных автором работах {\color{red}[]}.

\section{Предлагаемое решение.}


\subsection{Классификационные модели.}


\subsubsection{Метод ближайших соседей.}

\par
Метод взвешенных $k$ ближайших соседей - это метрический алгоритм классификации,
основанный на оценивании сходства объектов. Классифицируемый объект относится к тому классу,
к которому принадлежат ближайшие к нему объекты обучающей выборки. Более формально,
пусть на множестве объектов задана функция расстояния $\rho$, например, евклидова:
\[
    \rho(x, x') = \sum_i (x_i - x_i')^2.
\]
Для произвольного объекта $x \in \mathcal{X}$ расположим $k$ его ближайших соседей
из обучающей выборки в порядке возрастания расстояния до 
$x: \rho(x, x_{1;x}) \le \rho(x, x_{2;x}) \le ... \rho(x, x_{k;x})$.
Тогда формула для классификатора будет иметь вид:
\[
    A(x)=\arg\max_{y\in\mathcal{Y}} \sum_{i=1}^k[y_{i;x}=y]w(i, x),
\]
где $w(i, x)$ - заданная весовая функция, оценивающая важность $i$-ого соседа для
классификации объекта $x$. Существует множество эвристик для определения типа $w$,
самые простые из которых это: константная и обратно пропорциональная расстоянию до $i$-ого
объекта. Параметр $k$ данного метода подбирается для каждой задачи индивидуально.

\subsubsection{Случайный лес.}


\subsubsection{Метод опорных векторов.}


\subsubsection{Байесовский классификатор.}

Байесовский классификатор относит объект $x$ к классу $y$, 
вероятность которого для этого объекта максимальна:
\[
    y_i = \arg \max_{y\in\mathcal{Y}} P(y|x).
\]
По теореме Байеса:
\[
    P(y|x)=\frac{P(x|y)P(y)}{P(x)}.
\]
Знаменатель является константой, т.е. не влияет на результат классификации.
Далее, используя предположение о зависимости $x$ только от класса $y$, а не
от других объектов, перепишем числитель:
\[
    P(x_1, ..., x_n|y)P(Y) = P(Y)\prod_i P(x_i|y),
\]
где $x_i$ - признаковое описание объекта $x$. Итоговая формула классификатора имеет вид:
\[
    A(x)=\arg\max_{y\in\mathcal{Y}} P(y)\prod_i P(x_i|y).
\]

\subsubsection{Логистическая регрессия.}


\subsubsection{Градиентный бустинг.}


\subsection{Настройка классификационной модели.}


\subsubsection{Оценка качества классификации.}

\par
Для оценки качества построенных классификационных моделей будет использован стандартный
для данных задач подход - скользящий контроль по $q$ блокам (q-fold cross validation).
Рассмотрим данный метод подробнее. 
Пусть $\mathcal{X}$ - множество описаний объектов, 
$\mathcal{Y}$ - конечное множество меток классов. Заданы конечная обучающая выборка
\[
    X^L=(x_i, y_i)_{i=1}^L \subset \mathcal{X} \times \mathcal{Y} 
\]
и алгоритм обучения - отображение $\mu$, которое
произвольной конечной обучающей выборке $X \subset \mathcal{X}$
ставит в соответствие алгоритм $A:\ X \to \mathcal{Y}$.
Тогда выборка разбивается случаным образом на $q$ непересекающихся блоков размеров
$k_1,...,k_q$:
\[
    X^L=X_1^{k_1} \cup ... \cup X_q^{k_q}
\]
таких, что $k_1+...+k_q=L$. Каждый блок по очереди становится контрольной опдвыборкой,
при этом алгоритм обучения строится по оставшися $q-1$ блокам. Критерий качества
определяется как средняя ошибка на контрольной подвыборке:
\[
    CV(\mu, X^L)=\frac{1}{q}\sum_{n=1}^q Q(\mu(X^L \setminus X_n^{k_n}),X_n^{k_n}),
\]
где $Q(A, X)$ - некоторый функционал качества алгоритма $A$.
\par
В данной работе нас интересует процент правильно определенных меток классов,
поэтому в качестве функционала качества берется точность (accuracy),
определяемую по формуле:
\[
    ACC=\frac{TP+TN}{P+N},
\]
где TP (true positives) - истинно положительные, TN (true negative) - истинно отрицательные,
P (positive) - положительные, N (negative) - отрицательные ответы.
\par
Отдельно стоит рассмотреть способы разбиения обучающей выборки на блоки для скользящего
контроля. Стандартным подходом является равновероятное отнесение каждого примера выборки
к одному из $q$ классов. Однако этот подход может завышать истинное значение функционала
качества, если нам интересна его оценка на выборке, пространственно удалённой от заданной
обучающей. Причиной такого завышения является непрерывность данных, т.е. вероятность 
одинакового ответа двух объектов увеличивается с уменьшением этого расстояния.
Для исследования этого эффекта будем использовать разбиения объектов выборки
(географических точек поверхности Земли) двумя способами: полосами и квадратными блоками.
{\color{red} рис}. Плюсом первого подхода является моделирование оценки качества на
пространственно удаленной территории. Однако для данной задачи при таком разбиении
не для всех классов будут попадать объекты и в обучающую и в контрольную подвыборки.
Этого недостатка не имеет метод с разбиением на квадратные блоки, при правильном
подборе размера стороны такого блока.
\par

\subsubsection{Оценка важности признаков.}


\subsubsection{Подбор гиперпараметров модели.}

\par
Все используемые в работе классификационные модели имеют набор гиперпараметров, подбор которых
необходим для улучшения функционала качества. Оптимальные значения могут отличаться не только
для разных задач машинного обучения, но и на разных подзадачах, возникающих при классификации
типов почв по космическим снимкам. Будем использовать самый простой способ решения этой задачи - 
подбор гиперпараметров по заданнйо сетке. Основным недостатком этого метода являются
временные затраты в случае большого количества комбинаций параметров для перебора.
Однако для задач этой работы это оказывается не критичным. Опишем данный алгоритм подробнее.
\par
Пусть $p_1, ..., p_l$ - список настраеваемых параметров алгоритма обучения $\mu$.
Для каждого $p_i$ из них зададим список возможных значений $p_{i_{min}}, ..., p_{i_{max}}$,
где значения между максимальным и минимальным меняются либо с постоянным,
либо с логарифмическим шагом. Тогда оптимальными значениями $\bar{p_1}, ... \bar{p_l}$
определяются те, которые максимизируют оценку качества на скользящем контроле для
обучающей выобрки $X^L$:
\[
    \{\bar{p_i}\}_{i=1}^l=arg\max_{\{p_i\}} CV(\mu(\{p_i\}_{i=1}^l), X^L).
\]

\subsection{Данные для экспериментов.}

\par
Для исследований были взяты 34 фрагмента кадров спутников Landsat 5, 7, 8 
на территорию Павловского, Арсеньевского и Чернского районо Тульской области Российской Федерации.
Выбор именно этого спутника обусловлен тем, что программа Landsat является наиболее
продолжительным проектом по получению спутниковых снимков планеты. Так первый из спутников
был запущен в 1972 году, а последний на данный момент Landsat 8 - в 2013 году.
В данном исследовании нас интересует открытая поверхность почвы, которая на абсолютном
большинстве снимков исследуемой области покрыта облачным или снежным покровом.
По этой причине из нескольких сотен снимков за указанный промежуток времени были
отобраны только 35 штук. {\color{red} таблица}
\par
Сами спутники, количество и качество их сенсоров претерпевали изменения на протяжении
программы Landsat. 

\subsection{Модель линии почвы.}


\subsection{Предобработка снимков.}


\subsubsection{Фильтрация снимков.}

\par
Для задачи классификации типов почв по набору космических снимков не все пиксели этих снимков
имею одинковую пользу, а некоторые значения мешают построению признаковых описаний. В идеале
для решения задачи необходимо использовать только пиксели, попадающие в облаcть голой почвы
на каждом из снимков. В область голой почвы не входят:
\begin{itemize}
\item некоторые элементы ландшафта (реки, озера),
\item снежный покров,
\item облачный покров,
\item области вегетирующей в момент съемки растительности,
\item неприродные объекты (дороги, здания).
\end{itemize}
В данной работе для построения области голой почвы, была использована
модель спектральной окрестности линии почвы, описанная выше в разделе {\color{red} 3.*}.
\par
Построение алгоритма выделения спектральной окрестности линии почвы по распределению 
значений в каналах Red и Nir лежит за пределами исследований данной работы. Известные на данный
момент автоматические алгоритмы построения линии почвы, описанные, например, в {\color{red}[]},
оказываются не применимы в данной задаче, т.к. окрестность не дадается не самим уравнением линии
почвы, ни нимерьшими значениями Red при фиксированных Nir, как предлагается в этой статье.
По этим причинам данный шаг предобработки спутниковых снимков оставлен в полуавтоматическом режиме,
т.е. спектральная окрестность линии почвы выделяется экспертом почвоведом по графику распределения
значений, см. рис. {\color{red}*}.
\par
Заметим так же, что выделенная окрестность однозначно отображается в пространство объектов,
т.е. георафических точек поверхности Земли. Поэтому по той же пространственной области
фильтруются и другие каналы спутниковых снимков: Green, Blue и т.д.

\subsubsection{Нормализация снимков.}

\par
Как было указано в разделе {\color{red} 3.*} данные для исследований содержат спутниковые снимки
одной области за более 15 лет. Такой массив данных порождает проблемы нормализации этих снимков,
т.к. статистические характеристики распределений значений в их каналах существенно различаются.
Эти различия объясняются следующими основными факторами:
\begin{itemize}
\item различия сенсоров на спутниках разных версий,
\item различия примененных на спутниках постобработок значений,
\item различия условий освещения и параметров атмосферы в момент съемки.
\end{itemize}
\par
Единого метода нормализации космических снимков не существует, и для каждой задачи он
подбирается индивидуально. В статье {\color{red}[]} исследуются следующие методы нормализации
для уменьшения отклонений кусочно-линейных аппроксимаций спектральной окрестности 
разных снимков друг от друга:
\begin{itemize}
\item классическая нормализация (вычитание математического ожидания и деление на дисперсию),
\item последовательное применение поворота и сдвига,
\item последовательное применение атмосферной коррекции и сдвига,
\item атмосферная коррекция,
\item сдвиг,
\item поворот,
\item исходные данные (без нормализации).
\end{itemize}
Авторы приходят к выводу, что для данной задачи наилучшие результаты 
показывает классическая нормализация. На рис. {\color{red}* и *} приведены примеры
кусочно-линейных аппроксимаций линии почвы до и после использоваиния нормализации.

\subsubsection{Усреднение снимков.}

Важной проблемой при обработке цифровых снимков является подавление шумов,
неизбежно возникающих из-за несовершеноства снимающего сенсора и условий съемки.
Для подавления шумов используются такие методы как гауссовский, медианный или билатеральный
фильтр. В данной работе будет использоваться медианный фильтр, т.к. он не порождает новых значений,
а заменяет значение пикселя на медианут значений его пространственной окрестности.
Единственным параметром медианной фильтрации является радиус в пикселях пространственной
окрестности. Влияние радиуса усреднения на качество решения задачи классификации будет рассмотрено
а разделе {\color{red} 4.*}.

\subsection{Итоговое признаковое описание объектов.}

Признаковое описание для каждой исследуемой географической точки строится по всем накрывающим
её разноврменным космическим снимкам, прошедших 3 стадии предобработки: фильтрацию,
нормализацию и усреднение. В данной работе признаки берутся из множества статистических
характеристик распределений значений 4 цветовых каналов: Red, Nir, Green, Blue. Дополнительно
признаками являлются коэффициент наклона линии почвы временной и доля выборки, не принадлежащая
маске фильтрации. Приведем формулы вычисления всех 18 используемых признаков. Обозначим 
$M$ - множество индексов снимков, значения которых в данной точке не входит в маску фильтрации.
\begin{align*}
    E_{red} &= \frac{1}{|M|}\sum_{i \in M} red_i \\
    STD_{red} &= \frac{1}{|M|}\sum_{i \in M} (red_i - E_{red})^2 \\
    MIN_{red} &= \min_{i \in M} red_i \\
    MAX_{red} &= \max_{i \in M} red_i \\
\end{align*}
\begin{align*}
    E_{nir} &= \frac{1}{|M|}\sum_{i \in M} nir_i \\
    STD_{nir} &= \frac{1}{|M|}\sum_{i \in M} (nir_i - E_{nir})^2 \\
    MIN_{nir} &= \min_{i \in M} nir_i \\
    MAX_{nir} &= \max_{i \in M} nir_i \\
\end{align*}
\begin{align*}
    E_{green} &= \frac{1}{|M|}\sum_{i \in M} green_i \\
    STD_{green} &= \frac{1}{|M|}\sum_{i \in M} (green_i - E_{green})^2 \\
    MIN_{green} &= \min_{i \in M} green_i \\
    MAX_{green} &= \max_{i \in M} green_i \\
\end{align*}
\begin{align*}
    E_{blue} &= \frac{1}{|M|}\sum_{i \in M} blue_i \\
    STD_{blue} &= \frac{1}{|M|}\sum_{i \in M} (blue_i - E_{blue})^2 \\
    MIN_{blue} &= \min_{i \in M} blue_i \\
    MAX_{blue} &= \max_{i \in M} blue_i \\
\end{align*}
\begin{align*}
    A_{soil\_line} &= \frac{|M|\sum_{i \in M} red_i nir_i - 
                            \sum_{i \in M} red_i \sum_{i \in M} nir_i}
                           {|M|\sum_{i \in M} red_i^2 - 
                            \left(\sum_{i \in M} red_i\right)^2} \\
    N_{mask} &= |M|
\end{align*}

\section{Эксперименты.}


\subsection{Используемая вычислительная система.}


\subsection{Классификация по данным почвенных разрезов.}


\subsection{Классификация по данным почвенно карты.}


\section{Заключение.}


\section{Список литературы.}

\end{document}
