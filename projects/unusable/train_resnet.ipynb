{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import albumentations\n",
    "import pytorch_lightning\n",
    "from functools import partial\n",
    "from argparse import ArgumentParser, Namespace\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LambdaCallback\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from dataset import BaseDataModule\n",
    "from utils import generate_or_read_labels\n",
    "from pytorch_model import BaseModel, pytorch_transform, log_weights\n",
    "from runners.predict_pytorch import predict\n",
    "from pytorch_model_resnet import ResnetModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    'training_image_path': '/home/fila/data/soil_line/unusable/CH', \n",
    "    'validation_image_path': '/home/fila/data/soil_line/unusable/CH', \n",
    "    'shape_path': '/home/fila/data/soil_line/unusable/fields_v2/fields.shp', \n",
    "    'excel_path': '/home/fila/data/soil_line/unusable/fields_v2/flds_all_good.xls', \n",
    "    'log_path': '/home/soilnn/logs/unusable', \n",
    "    'n_training_batches': 100, \n",
    "    'n_validation_batches': 10, \n",
    "    'batch_size': 64, \n",
    "    'n_processes': 16, \n",
    "    'buffer_size': 1, \n",
    "    'buffer_update_size': 16, \n",
    "    'image_size': 128, \n",
    "    'resolution': 30.0\n",
    "}\n",
    "options = Namespace(**options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pytorch_lightning.Trainer(\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    max_epochs=120,\n",
    "    limit_train_batches=options.n_training_batches,\n",
    "    limit_val_batches=options.n_validation_batches,\n",
    "    val_check_interval=options.n_training_batches,\n",
    "    default_root_dir=options.log_path,\n",
    "    num_sanity_val_steps=0,\n",
    "    log_every_n_steps=10,\n",
    "    gradient_clip_val=0.5,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(every_n_epochs=12, filename='{epoch:02d}-{val_precision:.2f}', save_top_k=-1),\n",
    "        # LambdaCallback(on_after_backward=log_weights)  # uncomment for logging weights and grads\n",
    "    ]\n",
    ")\n",
    "fields = geopandas.read_file(options.shape_path).set_index('name')\n",
    "label_lambda = partial(\n",
    "    generate_or_read_labels,\n",
    "    excel_path=options.excel_path,\n",
    "    fields=fields\n",
    ")\n",
    "data_module = BaseDataModule(\n",
    "    training_labels=label_lambda(\n",
    "        image_path=options.training_image_path,\n",
    "        label_path=os.path.join(os.path.dirname(options.log_path), 'training_173174.csv')\n",
    "    ),\n",
    "    validation_labels=label_lambda(\n",
    "        image_path=options.validation_image_path,\n",
    "        label_path=os.path.join(os.path.dirname(options.log_path), 'validation_173174.csv')\n",
    "    ),\n",
    "    training_image_path=options.training_image_path,\n",
    "    validation_image_path=options.validation_image_path,\n",
    "    training_transform=partial(\n",
    "        pytorch_transform,\n",
    "        augmentation=albumentations.Compose([\n",
    "            albumentations.HorizontalFlip(),\n",
    "            albumentations.VerticalFlip()\n",
    "        ]),\n",
    "    ),\n",
    "    validation_transform=pytorch_transform,\n",
    "    fields=fields,\n",
    "    batch_size=options.batch_size,\n",
    "    n_processes=options.n_processes,\n",
    "    buffer_size=options.buffer_size,\n",
    "    buffer_update_size=options.buffer_update_size,\n",
    "    image_size=options.image_size,\n",
    "    resolution=options.resolution,\n",
    "    get_current_epoch=lambda: trainer.current_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soilnn/PythonProjects/unusable/venv/lib/python3.8/site-packages/torch/fx/graph.py:606: UserWarning: Attempted to insert a call_module Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule\n",
      "  warnings.warn(\"Attempted to insert a call_module Node with \"\n",
      "/home/soilnn/PythonProjects/unusable/venv/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResnetModel(\n",
       "  (model): Sequential(\n",
       "    (0): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Module(\n",
       "        (0): Module(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): Module(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Module(\n",
       "        (0): Module(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Module(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Module(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Module(\n",
       "        (0): Module(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Module(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Module(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Module(\n",
       "        (0): Module(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Module(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Module(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       "      (my_conv1): Conv2d(8, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (1): Linear(in_features=1000, out_features=1, bias=True)\n",
       "    (2): Sigmoid()\n",
       "  )\n",
       "  (accuracy): Accuracy()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResnetModel.load_from_checkpoint('/home/soilnn/logs/unusable/lightning_logs/version_113/checkpoints/epoch=119-val_precision=0.89.ckpt')\n",
    "model.eval()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_dir = '/home/soilnn/logs/unusable/lightning_logs/version_114/checkpoints'\n",
    "for ckpt in os.listdir(models_dir):\n",
    "    model_path = models_dir + '/' + ckpt\n",
    "    model = ResnetModel.load_from_checkpoint(model_path)\n",
    "    model.eval()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    \n",
    "    train_predictions = predict(model, options, options.training_image_path, label_path='/home/soilnn/logs/training_173174.csv')\n",
    "    result_path = model_path + '_results/'\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    train_predictions.to_csv(os.path.join(result_path, 'train_predictions_173174.csv'))\n",
    "    \n",
    "    # valid_predictions = predict(model, options, options.validation_image_path, label_path='/home/soilnn/logs/validation.csv')\n",
    "    # result_path = model_path + '_results/'\n",
    "    # if not os.path.exists(result_path):\n",
    "    #     os.makedirs(result_path)\n",
    "    # valid_predictions.to_csv(os.path.join(result_path, 'validation_predictions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = pd.read_csv('/home/soilnn/logs/unusable/lightning_logs/version_112/checkpoints/epoch=119-val_precision=0.91.ckpt_results/train_predictions_173174.csv', index_col=0)\n",
    "train_labels = data_module.training_labels\n",
    "valid_labels = data_module.validation_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust weights for bad scenes\n",
    "pred__ = train_predictions\n",
    "labels = train_labels\n",
    "\n",
    "file_names_false = data_module.trd.dataset.base_file_names[0]\n",
    "weights_false = ((pred__ > 0.55)[labels==0].sum(axis=1))**2 + 100\n",
    "weights_false = weights_false[file_names_false]\n",
    "weights_false = weights_false / weights_false.sum()\n",
    "\n",
    "file_names_true = data_module.trd.dataset.base_file_names[1]\n",
    "weights_true = ((pred__ < 0.5)[labels==1].sum(axis=1))**2 + 3\n",
    "weights_true = weights_true[file_names_true]\n",
    "weights_true = weights_true / weights_true.sum()\n",
    "\n",
    "column_weights_false = pred__[labels==0] ** 2 + 0.01\n",
    "column_weights_false = column_weights_false.loc[file_names_false]\n",
    "column_weights_false = column_weights_false.div(column_weights_false.sum(axis=1), axis=0)\n",
    "\n",
    "column_weights_true = (1 - pred__[labels==1]) ** 2 + 0.01\n",
    "column_weights_true = column_weights_true.loc[file_names_true]\n",
    "column_weights_true = column_weights_true.div(column_weights_true.sum(axis=1), axis=0)\n",
    "\n",
    "data_module.trd.dataset.base_file_name_weights[0] = weights_false.to_numpy()\n",
    "data_module.trd.dataset.column_weights[0] = column_weights_false\n",
    "data_module.trd.dataset.base_file_name_weights[1] = weights_true.to_numpy()\n",
    "data_module.trd.dataset.column_weights[1] = column_weights_true\n",
    "\n",
    "# self = data_module.trd.dataset\n",
    "# self.base_file_name_weights = [[1 / len(self.base_file_names[i]) for _ in self.base_file_names[i]] for i in\n",
    "#                                range(self.n_classes)]\n",
    "# self.column_weights = [\n",
    "#     pd.DataFrame(data=(self.labels.to_numpy() == i) / (self.labels.to_numpy() == i).sum(axis=1, keepdims=True),\n",
    "#                  index=self.labels.index,\n",
    "#                  columns=self.labels.columns) for i in range(self.n_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
